\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[margin=1in]{geometry}

\title{Triple-Sparse Architecture: Combining PEER, Engram, and mHC for\\Efficient Language Model Scaling}

\author{
  Anonymous Authors
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We present a triple-sparse architecture that combines three complementary mechanisms for language models: PEER (Parameter Efficient Expert Retrieval) for compute-efficient parameter scaling, Engram for n-gram pattern memory, and mHC (Manifold-constrained Hyper-Connections) for stable multi-stream residual connections. Our experiments on TinyLlama (1.1B parameters) demonstrate that PEER enables 7$\times$ parameter scaling (to 7.68B) while maintaining constant throughput (~26K tokens/second). Individual training results show: PEER-262K achieves -10.5\% perplexity, Engram achieves -31.7\%, and mHC achieves -25.7\%. However, joint training of all three components hits a ``Redundancy Bottleneck'' (-16.6\%), where PEER and Engram compete for the same information signal. We introduce \textbf{differential learning rate training}, which breaks this bottleneck by allowing modules to specialize sequentially, achieving \textbf{-33.2\% perplexity}---the best result overall. We provide comprehensive benchmarks, redundancy analysis, and detailed training strategies.
\end{abstract}

\section{Introduction}

Scaling language models has traditionally required proportional increases in both memory and compute. A model with 10$\times$ more parameters requires roughly 10$\times$ more FLOPs per forward pass, creating a fundamental barrier to efficient scaling. Recent work on Mixture of Experts (MoE) partially addresses this by activating only a subset of parameters per token, but standard MoE architectures are limited to dozens or hundreds of experts due to routing overhead.

PEER (Parameter Efficient Expert Retrieval) \cite{peer2024} breaks this constraint through product-key retrieval, enabling millions of micro-experts with $O(\sqrt{n})$ lookup complexity. However, the practical benefits of PEER for language model augmentation have not been thoroughly benchmarked, particularly regarding:

\begin{enumerate}
    \item \textbf{Compute efficiency}: Does throughput actually remain constant as experts scale?
    \item \textbf{Quality improvement}: Does training additional PEER capacity improve perplexity?
    \item \textbf{Complementary architectures}: How does PEER interact with other memory mechanisms?
\end{enumerate}

We address these questions by integrating PEER into TinyLlama and conducting systematic benchmarks. Additionally, we combine PEER with Engram \cite{engram2024}, a hash-based n-gram memory module, creating a \textit{dual-sparse architecture} where:

\begin{itemize}
    \item \textbf{Engram} (early layers): Retrieves static n-gram patterns for phrase-level recognition
    \item \textbf{PEER} (late layers): Routes to specialized micro-experts for contextual reasoning
\end{itemize}

Our key contributions:

\begin{enumerate}
    \item \textbf{Efficiency benchmarks}: We demonstrate 7$\times$ parameter scaling (1.1B $\rightarrow$ 7.68B) with $<$3\% throughput reduction
    \item \textbf{Training validation}: Trained PEER-262K achieves 14.81 perplexity vs 17.12 baseline (-10.5\%)
    \item \textbf{Triple-sparse architecture}: Combined PEER + Engram + mHC with differential LR achieves 11.05 perplexity (-33.2\%)
    \item \textbf{Redundancy Bottleneck discovery}: We identify and solve the ``Sparsity Overlap'' problem where joint training underperforms individual components
    \item \textbf{Differential LR training}: A phased training strategy that breaks the redundancy bottleneck
\end{enumerate}

\section{Related Work}

\subsection{Mixture of Experts}

Sparse Mixture of Experts \cite{shazeer2017outrageously} introduced the concept of conditional computation, where only a subset of model parameters are activated per input. GShard \cite{lepikhin2020gshard} and Switch Transformer \cite{fedus2022switch} scaled this approach to trillion-parameter models. However, these approaches typically use dozens to hundreds of experts with learned routing, limiting the granularity of specialization.

\subsection{Product Key Memory}

Product keys \cite{lample2019large} enable efficient retrieval from large memory banks through factorized key representations. Given $n$ memory slots, product keys use two sets of $\sqrt{n}$ keys, reducing lookup from $O(n)$ to $O(\sqrt{n})$. PEER \cite{peer2024} applies this principle to expert retrieval, enabling millions of micro-experts.

\subsection{N-gram Memory}

Engram \cite{engram2024} augments language models with hash-based n-gram retrieval, providing a non-parametric memory for common patterns. Unlike attention-based retrieval, Engram uses deterministic hashing for $O(1)$ lookup of phrase patterns.

\section{Method}

\subsection{Architecture Overview}

We augment a pretrained TinyLlama model (1.1B parameters, 22 layers) with two sparse memory modules:

\begin{itemize}
    \item \textbf{PEER layers} at positions 19 and 21 (late layers)
    \item \textbf{Engram layers} at positions 1 and 5 (early layers)
\end{itemize}

This placement follows the intuition that early layers handle pattern recognition while late layers handle reasoning.

\subsection{PEER Module}

Each PEER layer contains:

\begin{itemize}
    \item \textbf{Product keys}: Two sets of $\sqrt{n}$ keys, where $n$ is the number of experts
    \item \textbf{Expert embeddings}: $n$ pairs of down-projection and up-projection vectors
    \item \textbf{Query projection}: Maps hidden states to query vectors for each key set
\end{itemize}

For input hidden state $\mathbf{h} \in \mathbb{R}^d$:

\begin{enumerate}
    \item Project to queries: $\mathbf{q}_1, \mathbf{q}_2 = \text{split}(W_q \mathbf{h})$
    \item Compute similarities: $s_1 = \mathbf{q}_1 K_1^T$, $s_2 = \mathbf{q}_2 K_2^T$
    \item Select top-$k$ from each: $I_1 = \text{topk}(s_1)$, $I_2 = \text{topk}(s_2)$
    \item Form Cartesian product and select final top-$k$ experts
    \item Retrieve expert weights and compute: $\mathbf{o} = \sum_i \alpha_i \cdot \sigma(W_{\text{down}}^i \mathbf{h}) \cdot W_{\text{up}}^i$
\end{enumerate}

With $k=8$ experts per head and 4 heads, we retrieve 32 experts per token from pools of up to 803K experts.

\subsection{Engram Module}

Each Engram layer computes:

\begin{enumerate}
    \item Hash n-grams (2-gram, 3-gram) to indices: $i = \text{hash}(t_{-n+1:0}) \mod V$
    \item Lookup embeddings: $\mathbf{e} = E[i]$
    \item Compute gated output: $\mathbf{o} = \sigma(g(\mathbf{h}, \mathbf{e})) \cdot W_v \mathbf{e}$
\end{enumerate}

The gate $g$ uses the product of normalized keys and queries with sqrt-sign activation for stability.

\subsection{Initialization}

A critical finding: \textbf{new modules must be initialized near-zero} to avoid corrupting pretrained hidden states. We use:

\begin{verbatim}
nn.init.normal_(weight, std=0.01)  # projections
nn.init.normal_(embedding, std=0.02)  # embeddings
\end{verbatim}

Without this, Engram produced 450 perplexity (+2626\%) instead of 11.30 (-31.7\%).

\section{Experiments}

\subsection{Experimental Setup}

\textbf{Base model}: TinyLlama-1.1B-Chat-v1.0 \\
\textbf{Hardware}: NVIDIA RTX 5070 Ti (15.5 GB VRAM) \\
\textbf{Dataset}: WikiText-2 (train and test splits) \\
\textbf{Metrics}: Perplexity, peak VRAM, throughput (tokens/second)

\subsection{Efficiency Benchmark}

We measure inference efficiency across PEER configurations without training (random initialization). This isolates the computational overhead of PEER from any quality effects.

\begin{table}[h]
\centering
\caption{PEER Efficiency Scaling (Untrained)}
\begin{tabular}{lrrrrr}
\toprule
Config & Experts & Parameters & VRAM & Throughput & PPL \\
\midrule
Baseline & — & 1.10B & 2.07 GB & 26,991 tok/s & 17.12 \\
PEER-16K & 16,384 & 1.24B & 2.35 GB & 26,521 tok/s & 17.12 \\
PEER-65K & 65,536 & 1.64B & 3.10 GB & 26,405 tok/s & 17.12 \\
PEER-262K & 262,144 & 3.25B & 6.10 GB & 26,446 tok/s & 17.12 \\
PEER-590K & 589,824 & 5.93B & 11.10 GB & 26,401 tok/s & 17.12 \\
\textbf{PEER-803K} & \textbf{802,816} & \textbf{7.68B} & \textbf{14.35 GB} & \textbf{26,262 tok/s} & \textbf{17.12} \\
\bottomrule
\end{tabular}
\label{tab:efficiency}
\end{table}

\textbf{Key observation}: Throughput remains nearly constant (26,262-26,991 tok/s, $<$3\% variance) despite 7$\times$ parameter increase. Memory scales linearly as expected (expert embeddings are stored), but compute does not—validating PEER's core efficiency claim.

\subsection{Training Results}

We train PEER-262K for 3000 batches with SGD (momentum=0.9, lr=0.01) on WikiText-2 train split.

\begin{table}[h]
\centering
\caption{Quality Results (Trained Models)}
\begin{tabular}{lrrr}
\toprule
Model & Perplexity & Change & Parameters \\
\midrule
Baseline TinyLlama & 17.12 & — & 1.10B \\
+ Engram (trained) & 11.30 & -34.0\% & 1.15B \\
+ PEER-262K (trained) & 14.81 & -13.5\% & 3.25B \\
+ Both (Hybrid) & 11.06 & -35.4\% & 3.30B \\
\bottomrule
\end{tabular}
\label{tab:quality}
\end{table}

\textbf{Key observation}: Trained PEER-262K achieves 14.81 perplexity, a 13.5\% improvement over baseline. This demonstrates that PEER's additional capacity is not just stored—it is \textit{utilized} to improve model quality.

\subsection{Ablation: PEER Layer Placement}

We place PEER in late layers (19, 21) based on the hypothesis that early layers handle pattern matching while late layers handle reasoning. Engram is placed in early layers (1, 5) for complementary coverage.

\subsection{Triple-Sparse Training: The Redundancy Bottleneck}

When training PEER + Engram + mHC jointly, we observe a surprising result: the combined model (-16.6\%) underperforms individual components like Engram (-31.7\%) or mHC (-25.7\%). We term this the \textbf{Redundancy Bottleneck}.

\begin{table}[h]
\centering
\caption{Joint vs Individual Training}
\begin{tabular}{lrr}
\toprule
Configuration & Perplexity & Change \\
\midrule
Baseline & 16.54 & --- \\
Engram alone & 11.30 & -31.7\% \\
mHC alone & 12.29 & -25.7\% \\
PEER alone & 14.81 & -10.5\% \\
\textbf{Triple-Sparse (joint)} & \textbf{13.79} & \textbf{-16.6\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Root cause: Sparsity Overlap.} Analysis of output magnitudes reveals that Engram dominates (2.62:1 ratio vs PEER), effectively ``crowding out'' PEER's contribution. Both modules compete for the same information signal---literal pattern matching.

\subsection{Solution: Differential Learning Rate Training}

We propose \textbf{differential learning rate training} to break the redundancy bottleneck. Instead of freezing modules (which causes distribution shift), we modulate learning rates across phases:

\begin{enumerate}
    \item \textbf{Phase 1 (Engram-Lead)}: Engram LR = $10^{-3}$, PEER LR = $10^{-5}$, mHC LR = $10^{-3}$
    \item \textbf{Phase 2 (PEER-Catchup)}: PEER LR = $10^{-3}$, Engram LR = $10^{-5}$, mHC LR = $10^{-4}$
    \item \textbf{Phase 3 (Coordinate)}: All LR = $5 \times 10^{-4}$
\end{enumerate}

\begin{table}[h]
\centering
\caption{Training Strategy Comparison}
\begin{tabular}{lrrl}
\toprule
Strategy & Perplexity & Change & Notes \\
\midrule
Baseline & 16.54 & --- & --- \\
Joint training & 13.79 & -16.6\% & Redundancy bottleneck \\
Staggered (freeze) & 99.77 & +503\% & Distribution shift \\
\textbf{Differential LR} & \textbf{11.05} & \textbf{-33.2\%} & \textbf{Best result} \\
\bottomrule
\end{tabular}
\label{tab:training_strategies}
\end{table}

\textbf{Key insight}: Never fully freeze modules. Maintaining a minimal learning rate ($10^{-5}$) allows modules to adapt to each other while one ``leads'' and the other ``follows.''

\section{Analysis}

\subsection{Why Throughput Stays Constant}

PEER's constant throughput despite 7$\times$ parameter increase stems from:

\begin{enumerate}
    \item \textbf{Sparse retrieval}: Only $k=8$ experts activated per head regardless of pool size
    \item \textbf{Product-key efficiency}: $O(\sqrt{n})$ lookup instead of $O(n)$
    \item \textbf{Embedding-based experts}: Simple lookup + matmul, no routing networks
\end{enumerate}

The dominant cost is the base model's attention and MLP computation, which PEER augments rather than replaces.

\subsection{Why Training Improves Quality}

Untrained PEER maintains baseline perplexity because:
\begin{itemize}
    \item Small initialization ($\sigma=0.01$) means PEER output $\approx 0$ initially
    \item The model effectively ignores untrained PEER contributions
\end{itemize}

After training:
\begin{itemize}
    \item Experts specialize to different input patterns
    \item Product-key retrieval routes tokens to relevant experts
    \item The 262K experts provide fine-grained specialization impossible with standard MoE
\end{itemize}

\subsection{Complementary Architectures and Functional Overlap}

Engram and PEER were designed for different purposes:

\begin{itemize}
    \item \textbf{Engram}: Deterministic n-gram lookup, improves pattern recognition
    \item \textbf{PEER}: Learned expert routing, improves contextual reasoning
    \item \textbf{mHC}: Multi-stream residuals with manifold constraints, stabilizes training
\end{itemize}

However, our experiments reveal \textbf{functional overlap} between PEER and Engram. Both modules attempt to ``retrieve'' information based on the input---Engram via literal n-gram hashing, PEER via learned query-key matching. When trained jointly, Engram learns faster (simpler mechanism) and captures patterns that PEER would otherwise specialize in.

\textbf{Evidence of overlap}: Output magnitude analysis shows Engram contributing 2.62$\times$ more than PEER in joint training. Differential LR training forces PEER to specialize in patterns Engram \textit{cannot} capture (context-dependent, non-literal patterns), achieving 11.05 PPL---better than either alone.

\section{Discussion}

\subsection{Implications for Scaling}

Our results suggest a new scaling paradigm:

\begin{enumerate}
    \item Train a moderate base model (1-7B parameters)
    \item Add PEER layers with millions of experts
    \item Train only PEER parameters (faster, less memory)
    \item Result: Quality of larger model, inference cost of smaller model
\end{enumerate}

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Memory still scales}: While compute is constant, VRAM scales linearly with experts
    \item \textbf{Single dataset}: We only evaluate on WikiText-2; broader evaluation needed
    \item \textbf{Small base model}: TinyLlama may not represent behavior at larger scales
    \item \textbf{Limited training}: 3000 batches may not fully utilize 262K expert capacity
\end{itemize}

\subsection{Future Work}

\begin{itemize}
    \item Scale to 1M+ experts with CPU offloading or quantization
    \item Evaluate on diverse benchmarks (MMLU, HellaSwag, etc.)
    \item Compare PEER training efficiency vs full model training
    \item Investigate optimal layer placement and expert count
\end{itemize}

\section{Conclusion}

We present a triple-sparse architecture combining PEER, Engram, and mHC for efficient language model scaling. Our benchmarks demonstrate that PEER enables 7$\times$ parameter scaling with constant throughput, and that training these parameters yields meaningful quality improvements.

A key discovery is the \textbf{Redundancy Bottleneck}: joint training of multiple sparse modules can underperform individual components due to functional overlap. We solve this with \textbf{differential learning rate training}, which allows modules to specialize sequentially while maintaining coordination. This achieves \textbf{11.05 perplexity (-33.2\%)}, the best result across all configurations.

Our findings suggest:
\begin{enumerate}
    \item \textbf{Sparse modules compete}: Different sparse retrieval mechanisms can interfere when trained jointly
    \item \textbf{Training strategy matters}: The same architecture yields vastly different results (99.77 vs 11.05 PPL) depending on training approach
    \item \textbf{Differential LR enables coordination}: Modulating learning rates forces specialization while maintaining adaptability
\end{enumerate}

These results validate both PEER's efficiency claims and highlight the importance of training strategies for multi-component architectures.

\section*{Code Availability}

Code and trained weights are available at: \url{https://github.com/MikeyBeez/PEER}

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{peer2024}
Anonymous Authors.
\textit{Mixture of A Million Experts}.
arXiv:2407.04153, 2024.

\bibitem{engram2024}
DeepSeek AI.
\textit{Engram: Byte-Level Memory-Augmented Language Modeling}.
GitHub, 2024.

\bibitem{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.
\textit{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}.
ICLR, 2017.

\bibitem{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\textit{GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding}.
ICLR, 2021.

\bibitem{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\textit{Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}.
JMLR, 2022.

\bibitem{lample2019large}
Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou.
\textit{Large Memory Layers with Product Keys}.
NeurIPS, 2019.

\bibitem{mhc2024}
DeepSeek AI.
\textit{Manifold-Constrained Hyper-Connections for Stable Deep Learning}.
arXiv:2512.24880, 2024.

\end{thebibliography}

\appendix

\section{mHC: Manifold-Constrained Hyper-Connections}
\label{app:mhc}

In addition to PEER and Engram, we implement mHC (Manifold-constrained Hyper-Connections) \cite{mhc2024}, which replaces standard residual connections with multi-stream hyper-connections. This creates a \textit{triple-sparse architecture} where each component serves a distinct purpose.

\subsection{mHC Architecture}

mHC expands hidden states into $N$ parallel streams and uses learned mixing matrices projected to doubly-stochastic form via Sinkhorn-Knopp iteration.

For input $\mathbf{x} \in \mathbb{R}^{B \times T \times D}$:

\begin{enumerate}
    \item \textbf{Stream Expansion}: $\mathbf{x}_{\text{exp}} = W_{\text{expand}} \mathbf{x} \in \mathbb{R}^{B \times T \times N \times D}$
    \item \textbf{Compute mixing weights} from flattened input:
    \begin{itemize}
        \item Pre-mixing: $W_{\text{pre}} = \sigma(\alpha_{\text{pre}} + \tanh(P_{\text{pre}} \mathbf{x}_{\text{flat}}))$
        \item Post-mixing: $W_{\text{post}} = 2\sigma(\alpha_{\text{post}} + \tanh(P_{\text{post}} \mathbf{x}_{\text{flat}}))$
        \item Residual mixing: $W_{\text{res}} = \text{Sinkhorn}(\exp(P_{\text{res}} \mathbf{x}_{\text{flat}}))$
    \end{itemize}
    \item \textbf{Apply inner function} (attention or MLP): $\mathbf{y} = f(W_{\text{pre}} \mathbf{x}_{\text{exp}})$
    \item \textbf{Combine}: $\mathbf{o} = W_{\text{res}} \mathbf{x}_{\text{exp}} + W_{\text{post}} \mathbf{y}$
    \item \textbf{Stream Reduction}: $\mathbf{x}_{\text{out}} = W_{\text{reduce}} \mathbf{o} \in \mathbb{R}^{B \times T \times D}$
\end{enumerate}

\subsection{Sinkhorn-Knopp Projection}

The residual mixing matrix $W_{\text{res}}$ is projected to doubly-stochastic form (rows and columns sum to 1) via iterative normalization:

\begin{algorithm}
\caption{Sinkhorn-Knopp Projection}
\begin{algorithmic}[1]
\REQUIRE Matrix $A \in \mathbb{R}^{N \times N}$, iterations $K$
\STATE $A \leftarrow \exp(A)$ \COMMENT{Ensure non-negative}
\FOR{$k = 1$ to $K$}
    \STATE $A \leftarrow A / \sum_j A_{ij}$ \COMMENT{Row normalization}
    \STATE $A \leftarrow A / \sum_i A_{ij}$ \COMMENT{Column normalization}
\ENDFOR
\RETURN $A$
\end{algorithmic}
\end{algorithm}

Doubly-stochastic matrices preserve signal magnitude, preventing gradient explosion/vanishing in deep networks.

\subsection{mHC Integration Results}

We apply mHC to layers 5, 11, and 17 (spread across the 22-layer model) with $N=4$ streams.

\begin{table}[h]
\centering
\caption{mHC Training Results}
\begin{tabular}{lrrr}
\toprule
Model & Perplexity & Change & Parameters \\
\midrule
Baseline TinyLlama & 16.54 & — & 1.10B \\
+ mHC (trained) & 12.29 & \textbf{-25.7\%} & 1.21B \\
\bottomrule
\end{tabular}
\label{tab:mhc}
\end{table}

\textbf{Key finding}: mHC achieves \textbf{25.7\% perplexity improvement}, demonstrating that multi-stream residual connections with manifold constraints significantly enhance model quality.

\subsection{mHC Implementation Details}

Our mHC wrapper applies processing \textit{after} the decoder layer rather than replacing it:

\begin{verbatim}
class mHCLayer(nn.Module):
    def forward(self, hidden_states):
        x_exp = self.expander(hidden_states)
        x_mhc = self.mhc(x_exp)
        x_red = self.reducer(x_mhc)
        mhc_out = self.out_proj(x_red)
        gate = sigmoid(self.gate_scale)  # ~0.018
        return hidden_states + gate * mhc_out
\end{verbatim}

Critical design choices:
\begin{itemize}
    \item \textbf{Gated output}: Gate initialized to $\sim$0.018 via \texttt{gate\_scale = -4.0}
    \item \textbf{Small projection init}: \texttt{std=0.001} for output projection
    \item \textbf{Post-layer application}: mHC processes layer output, not input
\end{itemize}

\section{Complete Results Summary}
\label{app:results}

\begin{table}[h]
\centering
\caption{All Training Results on WikiText-2}
\begin{tabular}{lrrrr}
\toprule
Model & Perplexity & Change & Params & Throughput \\
\midrule
Baseline TinyLlama & 16.54 & — & 1.10B & 26,991 tok/s \\
\midrule
\multicolumn{5}{l}{\textit{Single Components}} \\
+ Engram & 11.30 & -31.7\% & 1.15B & $\sim$26K tok/s \\
+ PEER-262K & 14.81 & -10.5\% & 3.25B & 26,446 tok/s \\
+ mHC & 12.29 & -25.7\% & 1.21B & $\sim$25K tok/s \\
\midrule
\multicolumn{5}{l}{\textit{Dual Combinations}} \\
+ Engram + PEER & 11.06 & -33.1\% & 3.30B & $\sim$26K tok/s \\
\midrule
\multicolumn{5}{l}{\textit{Triple-Sparse (PEER + Engram + mHC)}} \\
Joint training & 13.79 & -16.6\% & 3.41B & $\sim$25K tok/s \\
Staggered (freeze) & 99.77 & +503\% & 3.41B & --- \\
\textbf{Differential LR} & \textbf{11.05} & \textbf{-33.2\%} & \textbf{3.41B} & $\sim$25K tok/s \\
\bottomrule
\end{tabular}
\label{tab:all_results}
\end{table}

\section{Training Hyperparameters}
\label{app:hyperparams}

\begin{table}[h]
\centering
\caption{Training Hyperparameters}
\begin{tabular}{lccc}
\toprule
Parameter & Engram & PEER-262K & mHC \\
\midrule
Optimizer & AdamW & SGD & AdamW \\
Learning rate (new params) & $10^{-3}$ & $10^{-2}$ & $10^{-3}$ \\
Learning rate (backbone) & $10^{-6}$ & $10^{-8}$ & $10^{-6}$ \\
Momentum & 0.9 (Adam $\beta$) & 0.9 & 0.9 (Adam $\beta$) \\
Weight decay & 0.01 & 0 & 0.01 \\
Batch size & 4 & 4 & 4 \\
Sequence length & 256 & 256 & 256 \\
Training batches & 1,500 & 3,000 & 1,500 \\
Gradient clipping & 1.0 & 1.0 & 1.0 \\
Precision & bfloat16 & bfloat16 & bfloat16 \\
\bottomrule
\end{tabular}
\label{tab:hyperparams}
\end{table}

\subsection{Why Different Optimizers?}

PEER-262K uses SGD instead of AdamW because:
\begin{itemize}
    \item AdamW maintains 2$\times$ parameter memory for optimizer states
    \item With 262K experts (2.15B additional parameters), this exhausted 15.5GB VRAM
    \item SGD with momentum requires only 1$\times$ additional memory
\end{itemize}

\subsection{Differential LR Training Schedule}

\begin{table}[h]
\centering
\caption{Differential Learning Rate Training Phases}
\begin{tabular}{lcccc}
\toprule
Phase & Batches & Engram LR & PEER LR & mHC LR \\
\midrule
1. Engram-Lead & 1,000 & $10^{-3}$ & $10^{-5}$ & $10^{-3}$ \\
2. PEER-Catchup & 1,000 & $10^{-5}$ & $10^{-3}$ & $10^{-4}$ \\
3. Coordinate & 1,000 & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ \\
\bottomrule
\end{tabular}
\label{tab:diff_lr}
\end{table}

\textbf{Key insight}: The minimum learning rate of $10^{-5}$ (not zero) is critical. Completely freezing modules causes distribution shift, leading to catastrophic forgetting (99.77 PPL). Maintaining a small learning rate allows modules to adapt to each other's changing outputs while one ``leads'' the optimization.

\section{Initialization Details}
\label{app:init}

Proper initialization is critical when augmenting pretrained models. Default PyTorch initialization produces high-variance outputs that corrupt carefully tuned hidden states.

\begin{table}[h]
\centering
\caption{Initialization Standard Deviations}
\begin{tabular}{lcc}
\toprule
Component & Weight Type & Std Dev \\
\midrule
\multicolumn{3}{l}{\textit{PEER}} \\
Query projection & Linear & 0.02 \\
Product keys & Parameter & 0.02 \\
Expert embeddings & Embedding & 0.01 \\
\midrule
\multicolumn{3}{l}{\textit{Engram}} \\
Value projection & Linear & 0.01 \\
Key projection & Linear & 0.01 \\
N-gram embeddings & Embedding & 0.02 \\
Convolution & Conv1d & 0 (zero init) \\
\midrule
\multicolumn{3}{l}{\textit{mHC}} \\
Stream expander & Linear & 0.02 \\
Stream reducer & Linear & 0.02 \\
$\alpha$ parameters & Scalar & 0.01 \\
Mixing projections & Linear & 0.01 \\
Output projection & Linear & 0.001 \\
Gate scale & Scalar & -4.0 (logit) \\
\bottomrule
\end{tabular}
\label{tab:init}
\end{table}

\subsection{Initialization Ablation}

\begin{table}[h]
\centering
\caption{Effect of Initialization on Engram}
\begin{tabular}{lrr}
\toprule
Initialization & Perplexity & Change \\
\midrule
Default PyTorch & 450.0 & +2,621\% \\
Small init (std=0.01-0.02) & 11.30 & -31.7\% \\
\bottomrule
\end{tabular}
\label{tab:init_ablation}
\end{table}

\section{PEER Module Algorithm}
\label{app:peer_algorithm}

\begin{algorithm}
\caption{PEER Forward Pass}
\begin{algorithmic}[1]
\REQUIRE Hidden state $\mathbf{h} \in \mathbb{R}^{B \times T \times D}$
\REQUIRE Keys $K_1, K_2 \in \mathbb{R}^{H \times \sqrt{n} \times d_k}$
\REQUIRE Expert embeddings $E_{\text{down}}, E_{\text{up}} \in \mathbb{R}^{n \times D}$
\STATE $\mathbf{q} \leftarrow W_q \mathbf{h}$ \COMMENT{Project to queries}
\STATE $\mathbf{q}_1, \mathbf{q}_2 \leftarrow \text{split}(\mathbf{q})$ \COMMENT{Split for product keys}
\FOR{each head $h$}
    \STATE $s_1 \leftarrow \mathbf{q}_1^h (K_1^h)^T$ \COMMENT{Similarities to key set 1}
    \STATE $s_2 \leftarrow \mathbf{q}_2^h (K_2^h)^T$ \COMMENT{Similarities to key set 2}
    \STATE $I_1, v_1 \leftarrow \text{topk}(s_1, k)$ \COMMENT{Top-k indices and scores}
    \STATE $I_2, v_2 \leftarrow \text{topk}(s_2, k)$
    \STATE $I_{\text{prod}} \leftarrow I_1 \times \sqrt{n} + I_2$ \COMMENT{Cartesian product}
    \STATE $v_{\text{prod}} \leftarrow v_1 + v_2$ \COMMENT{Combined scores}
    \STATE $I_{\text{final}}, v_{\text{final}} \leftarrow \text{topk}(I_{\text{prod}}, v_{\text{prod}}, k)$
    \STATE $W_{\text{down}} \leftarrow E_{\text{down}}[I_{\text{final}}]$
    \STATE $W_{\text{up}} \leftarrow E_{\text{up}}[I_{\text{final}}]$
    \STATE $\mathbf{a} \leftarrow \text{ReLU}(v_{\text{final}})$ \COMMENT{Non-competing scores}
    \STATE $\mathbf{o}^h \leftarrow \sum_i \mathbf{a}_i \cdot \sigma(W_{\text{down}}^i \mathbf{h}) \cdot W_{\text{up}}^i$
\ENDFOR
\RETURN $\sum_h \mathbf{o}^h$
\end{algorithmic}
\end{algorithm}

\section{Engram Module Algorithm}
\label{app:engram_algorithm}

\begin{algorithm}
\caption{Engram Forward Pass}
\begin{algorithmic}[1]
\REQUIRE Hidden state $\mathbf{h} \in \mathbb{R}^{B \times T \times D}$
\REQUIRE Input token IDs $\mathbf{t} \in \mathbb{Z}^{B \times T}$
\REQUIRE N-gram embeddings $E_2, E_3 \in \mathbb{R}^{V \times d_e}$
\STATE \COMMENT{Hash 2-grams}
\STATE $i_2 \leftarrow (\mathbf{t} \oplus (\mathbf{t}_{\text{shift}} \times m_1)) \mod V$
\STATE $\mathbf{e}_2 \leftarrow E_2[i_2]$
\STATE \COMMENT{Hash 3-grams}
\STATE $i_3 \leftarrow (i_2 \oplus (\mathbf{t}_{\text{shift2}} \times m_2)) \mod V$
\STATE $\mathbf{e}_3 \leftarrow E_3[i_3]$
\STATE $\mathbf{e} \leftarrow \text{concat}(\mathbf{e}_2, \mathbf{e}_3)$
\STATE \COMMENT{Compute gate}
\STATE $\mathbf{k} \leftarrow W_k \mathbf{e}$
\STATE $g \leftarrow (\text{RMSNorm}(\mathbf{k}) \cdot \text{RMSNorm}(\mathbf{h})) / \sqrt{D}$
\STATE $g \leftarrow \sigma(\sqrt{|g|} \cdot \text{sign}(g))$ \COMMENT{Sqrt-sign activation}
\STATE \COMMENT{Gated output}
\STATE $\mathbf{v} \leftarrow g \cdot W_v \mathbf{e}$
\STATE $\mathbf{v} \leftarrow \mathbf{v} + \text{SiLU}(\text{Conv1d}(\mathbf{v}))$
\RETURN $\mathbf{h} + \mathbf{v}$
\end{algorithmic}
\end{algorithm}

\section{Hardware and Reproducibility}
\label{app:hardware}

\begin{table}[h]
\centering
\caption{Hardware Configuration}
\begin{tabular}{ll}
\toprule
Component & Specification \\
\midrule
GPU & NVIDIA RTX 5070 Ti \\
VRAM & 15.5 GB GDDR7 \\
CUDA Version & 12.x \\
PyTorch Version & 2.x \\
Transformers & 4.x \\
\bottomrule
\end{tabular}
\label{tab:hardware}
\end{table}

\subsection{Reproducibility Checklist}

\begin{itemize}
    \item[$\checkmark$] Random seeds set for reproducibility
    \item[$\checkmark$] All hyperparameters documented
    \item[$\checkmark$] Training/evaluation code publicly available
    \item[$\checkmark$] Pretrained weights available for download
    \item[$\checkmark$] Dataset (WikiText-2) publicly available
\end{itemize}

\section{Limitations and Future Work}
\label{app:limitations}

\subsection{Current Limitations}

\begin{enumerate}
    \item \textbf{Single evaluation dataset}: WikiText-2 is n-gram heavy, which may favor Engram
    \item \textbf{Small base model}: TinyLlama (1.1B) may not represent larger model behavior
    \item \textbf{No downstream task evaluation}: Only perplexity measured, not task performance
    \item \textbf{Differential LR sensitivity}: Optimal phase durations and LR ratios may vary across architectures
    \item \textbf{Limited training budget}: Longer training may yield further improvements
\end{enumerate}

\subsection{Future Directions}

\begin{enumerate}
    \item \textbf{Downstream tasks}: Evaluate on MMLU, HellaSwag, ARC, etc.
    \item \textbf{Larger base models}: Test on 7B+ parameter models
    \item \textbf{Expert scaling}: Push to 1M+ experts with memory optimization
    \item \textbf{Continuous pretraining}: Train on larger corpora (RedPajama, etc.)
    \item \textbf{Differential LR optimization}: Tune phase durations and LR schedules for different architectures
    \item \textbf{Redundancy analysis}: Develop automated methods to detect and prevent module overlap
\end{enumerate}

\end{document}
