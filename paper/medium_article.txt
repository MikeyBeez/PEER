The Redundancy Bottleneck: Why Sparse Modules Compete and How to Coordinate Them


THE CORE FINDING

Sparse retrieval modules are not additive—they compete for representational real estate (gradient signal and explanatory responsibility under a shared loss).

If you combine multiple sparse mechanisms and train them jointly, the faster-learning module will monopolize the gradient signal, crowding out slower modules before they can specialize. The combined system underperforms its components.

We call this the Redundancy Bottleneck.


THE MECHANISM: LEARNING-SPEED ASYMMETRY

The Redundancy Bottleneck is fundamentally an optimization-time phenomenon.

When multiple modules can reduce the same training loss, gradient descent allocates explanatory responsibility to the module with the shortest effective path from input to loss. Simpler or more direct mechanisms learn faster, monopolizing the gradient signal early in training.

As a result:
- The faster-learning module collapses the error signal
- Slower or higher-capacity modules receive diminished gradients
- Latent capacity remains unused, even if it is expressive

This is not unique to sparse retrieval. It applies to any modular system where components overlap functionally but differ in learning speed: MoE routing, stacked adapters, retrieval-augmented models, hybrid memory architectures.

The failure mode is invisible in isolation and only emerges during joint optimization.


THE EXPERIMENT

We built a "triple-sparse architecture" combining three mechanisms:

PEER (Parameter Efficient Expert Retrieval):
- Product-key lookup over 262,000+ micro-experts
- O(sqrt(n)) retrieval complexity
- Provides massive capacity without proportional compute

Engram (N-gram Pattern Memory):
- Hash-based lookup of 2-gram and 3-gram patterns
- O(1) deterministic retrieval
- Direct path: hash → embedding → output

mHC (Manifold-constrained Hyper-Connections):
- Multi-stream residual connections
- Doubly-stochastic mixing matrices
- Hypothesized to act as "representational bandwidth multiplier"

Each module works great individually:
- Engram alone: -31.7% perplexity
- mHC alone: -25.7% perplexity
- PEER alone: -10.5% perplexity

But trained together? The triple-sparse system achieves only -16.6%. That's worse than Engram alone.


THE ROOT CAUSE: FUNCTIONAL OVERLAP

Functional overlap occurs when multiple modules can independently explain the same variance in the training objective.

Both PEER and Engram attempt to "retrieve" information based on input patterns. But Engram's path (hash → embedding → output) is simpler than PEER's (query projection → product-key search → expert retrieval → output).

Due to learning-speed asymmetry, Engram dominates early optimization. Output magnitude analysis confirms: Engram contributes 2.62x more than PEER in joint training.

Engram captures patterns that PEER would otherwise specialize in. PEER's capacity goes unused.


THE SOLUTION: DIFFERENTIAL LEARNING RATE TRAINING

The fix isn't architectural—it's a training coordination principle.

Instead of training all modules equally, we let them take turns leading:

Phase 1 - Engram Leads (1,000 batches):
- Engram LR: 1e-3 (high)
- PEER LR: 1e-5 (minimal, but not zero)
- mHC LR: 1e-3 (high)

Phase 2 - PEER Catches Up (1,000 batches):
- PEER LR: 1e-3 (high)
- Engram LR: 1e-5 (minimal)
- mHC LR: 1e-4 (medium)

Phase 3 - Coordination (1,000 batches):
- All modules: 5e-4 (medium)

Critically, these phases are soft biases in learning speed, not strict stages. All modules remain plastic throughout training. The "following" module still adapts at 1e-5 LR, maintaining compatibility with the leader's evolving representations.

Never fully freeze. Complete freezing causes distribution shift—the frozen module becomes incompatible with the still-training modules.


THE RESULTS

Training Strategy          Perplexity    Change
Baseline                   16.54         —
Joint training (equal LR)  13.79         -16.6%    (Redundancy bottleneck)
Staggered (freeze modules) 99.77         +503%     (Distribution shift)
Differential LR            11.05         -33.2%    (Best result)

The same architecture yields vastly different results depending purely on training dynamics.


WHY mHC MATTERS

mHC achieves -25.7% improvement, but its mechanism is less obvious than Engram or PEER.

We hypothesize that mHC serves as a representational bandwidth multiplier: by expanding hidden states into multiple streams with doubly-stochastic mixing, mHC increases the effective dimensionality available for sparse modules to write into.

This may prevent early dominance from becoming irreversible—if Engram saturates one stream, PEER can still specialize in another. Gradient analysis shows mHC layers maintain significant gradient flow (0.73 mean magnitude), suggesting active participation in optimization rather than mere stabilization.


PEER AS CAPACITY INSURANCE

A skeptical question: "Why not just use Engram alone?"

PEER's immediate gain (-10.5%) is modest relative to its parameter cost (+2.15B). But PEER provides something Engram cannot: compute-decoupled scaling.

Adding more Engram vocabulary increases both memory AND computation.
Adding more PEER experts increases only memory—compute stays constant.

Our efficiency benchmark proves this:

Configuration    Parameters    Throughput
Baseline         1.10B         26,991 tok/s
PEER-16K         1.24B         26,521 tok/s
PEER-65K         1.64B         26,405 tok/s
PEER-262K        3.25B         26,446 tok/s
PEER-803K        7.68B         26,262 tok/s

7x parameters, <3% throughput reduction.

PEER is capacity insurance—scaling headroom that doesn't bottleneck inference. Its value isn't immediate quality; it's future specialization potential.

The Redundancy Bottleneck finding clarifies WHEN PEER's capacity becomes useful: on patterns Engram cannot capture (context-dependent, non-literal associations). Differential LR training ensures PEER learns these instead of redundantly duplicating Engram's work.


LIMITATIONS

Dataset bias: WikiText-2's repetitive n-gram structure inflates Engram's contribution. Cross-dataset evaluation on C4 shows Engram's gains diminish on diverse web content (+0.6% vs -31.7%). The Redundancy Bottleneck generalizes, but optimal schedules are dataset-dependent.

Differential LR sensitivity: The hyperparameters are hand-tuned. Re-tuning is needed for different model sizes and module combinations.

Small base model: Results on TinyLlama (1.1B) may not represent behavior at larger scales.


THE BROADER PRINCIPLE

Training-aware architecture design is essential.

Evaluating architectures without considering training dynamics misses half the story. The Redundancy Bottleneck is invisible in isolation—it only emerges during joint optimization.

Any multi-component system should:
1. Test individual components AND joint training
2. Use output magnitude analysis to diagnose dominance
3. Consider differential LR or other coordination strategies

The modules aren't the problem. The interaction under gradient descent is.


KEY TAKEAWAYS

1. Sparse modules compete for representational real estate (gradient signal + explanatory responsibility)

2. Learning-speed asymmetry causes the module with the shortest path to loss to dominate early optimization

3. Functional overlap means multiple modules can explain the same variance—the faster one wins

4. Differential LR training breaks the race by giving each module a protected learning window

5. Never fully freeze—maintain minimal LR for adaptation

6. Architecture is not destiny; training dynamics determine outcomes


THE GENERALIZATION

This applies beyond PEER + Engram + mHC:

- MoE routing competing with dense layers
- RAG retrieval competing with parametric memory
- Stacked adapters competing for the same features
- Any hybrid system with overlapping function

When you see a modular system underperform its components, suspect the Redundancy Bottleneck. Diagnose with magnitude analysis. Fix with differential LR.


CODE AVAILABILITY

Code and trained weights: https://github.com/MikeyBeez/PEER


REFERENCES

- PEER: Mixture of A Million Experts (arxiv.org/abs/2407.04153)
- Engram: Byte-Level Memory-Augmented Language Modeling (github.com/deepseek-ai/Engram)
- mHC: Manifold-Constrained Hyper-Connections (arxiv.org/abs/2512.24880)
