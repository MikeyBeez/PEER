\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[margin=1in]{geometry}

\title{The Redundancy Bottleneck: Why Sparse Modules Compete\\and How to Coordinate Them}

\author{
  Anonymous Authors
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
\textbf{Sparse retrieval modules are not additive---they compete for representational real estate.} We demonstrate this through a triple-sparse architecture combining PEER (Parameter Efficient Expert Retrieval), Engram (n-gram pattern memory), and mHC (manifold-constrained hyper-connections). While each module improves perplexity individually (Engram: -31.7\%, mHC: -25.7\%, PEER: -10.5\%), joint training produces a ``Redundancy Bottleneck'' where the combined model (-16.6\%) underperforms its components. Analysis reveals the cause: Engram dominates gradient flow (2.62:1 magnitude ratio), crowding out PEER's specialization. We introduce \textbf{differential learning rate training}---a coordination principle where modules take turns leading optimization while others follow at minimal learning rates. This achieves \textbf{-33.2\% perplexity}, the best result across all configurations. Our findings generalize beyond this specific architecture: any system combining multiple sparse retrieval mechanisms must coordinate their training to prevent functional overlap. We also validate PEER's efficiency claims, demonstrating 7$\times$ parameter scaling (1.1B $\rightarrow$ 7.68B) with $<$3\% throughput reduction.
\end{abstract}

\section{Introduction}

A natural assumption in modular deep learning is that capabilities are additive: if module A improves performance by 30\% and module B improves it by 10\%, combining them should yield roughly 40\% improvement. This paper demonstrates that \textbf{this assumption fails for sparse retrieval modules}---and explains why.

We combine three sparse mechanisms into a ``triple-sparse architecture'':
\begin{itemize}
    \item \textbf{PEER} \cite{peer2024}: Product-key retrieval over millions of micro-experts
    \item \textbf{Engram} \cite{engram2024}: Hash-based n-gram pattern memory
    \item \textbf{mHC} \cite{mhc2024}: Manifold-constrained multi-stream residuals
\end{itemize}

Each module individually improves perplexity substantially. But when trained jointly, performance \textit{degrades}---the combined model (-16.6\%) underperforms Engram alone (-31.7\%). We call this the \textbf{Redundancy Bottleneck}.

The root cause is \textbf{functional overlap}: both PEER and Engram attempt to ``retrieve'' information based on input patterns. During joint training, the faster-learning module (Engram) captures patterns that the slower module (PEER) would otherwise specialize in. Output magnitude analysis confirms this: Engram dominates at a 2.62:1 ratio, effectively crowding out PEER's contribution.

The solution is not architectural---it's a \textbf{training coordination principle}. We introduce \textit{differential learning rate training}, where modules take turns leading optimization:
\begin{enumerate}
    \item \textbf{Phase 1}: Engram leads (high LR), PEER follows (minimal LR)
    \item \textbf{Phase 2}: PEER leads, Engram follows
    \item \textbf{Phase 3}: Both coordinate at medium LR
\end{enumerate}

This achieves -33.2\% perplexity---better than any individual module or naive joint training.

\textbf{Key insight}: The same architecture yields vastly different results (99.77 PPL with frozen-phase training vs 11.05 PPL with differential LR) depending purely on training dynamics. This suggests that \textit{training-aware architecture design} is essential for multi-component systems.

We also validate PEER's efficiency claims independently: 7$\times$ parameter scaling (1.1B $\rightarrow$ 7.68B) with $<$3\% throughput reduction. PEER's value is \textit{capacity insurance}---it provides scaling headroom rather than immediate quality gains.

\subsection{Key Contributions}

\begin{enumerate}
    \item \textbf{Efficiency benchmarks}: We demonstrate 7$\times$ parameter scaling (1.1B $\rightarrow$ 7.68B) with $<$3\% throughput reduction
    \item \textbf{Training validation}: Trained PEER-262K achieves 14.81 perplexity vs 17.12 baseline (-10.5\%)
    \item \textbf{Triple-sparse architecture}: Combined PEER + Engram + mHC with differential LR achieves 11.05 perplexity (-33.2\%)
    \item \textbf{Redundancy Bottleneck discovery}: We identify and solve the ``Sparsity Overlap'' problem where joint training underperforms individual components
    \item \textbf{Differential LR training}: A phased training strategy that breaks the redundancy bottleneck
\end{enumerate}

\section{Related Work}

\subsection{Mixture of Experts}

Sparse Mixture of Experts \cite{shazeer2017outrageously} introduced the concept of conditional computation, where only a subset of model parameters are activated per input. GShard \cite{lepikhin2020gshard} and Switch Transformer \cite{fedus2022switch} scaled this approach to trillion-parameter models. However, these approaches typically use dozens to hundreds of experts with learned routing, limiting the granularity of specialization.

\subsection{Product Key Memory}

Product keys \cite{lample2019large} enable efficient retrieval from large memory banks through factorized key representations. Given $n$ memory slots, product keys use two sets of $\sqrt{n}$ keys, reducing lookup from $O(n)$ to $O(\sqrt{n})$. PEER \cite{peer2024} applies this principle to expert retrieval, enabling millions of micro-experts.

\subsection{N-gram Memory}

Engram \cite{engram2024} augments language models with hash-based n-gram retrieval, providing a non-parametric memory for common patterns. Unlike attention-based retrieval, Engram uses deterministic hashing for $O(1)$ lookup of phrase patterns.

\section{Method}

\subsection{Architecture Overview}

We augment a pretrained TinyLlama model (1.1B parameters, 22 layers) with two sparse memory modules:

\begin{itemize}
    \item \textbf{PEER layers} at positions 19 and 21 (late layers)
    \item \textbf{Engram layers} at positions 1 and 5 (early layers)
\end{itemize}

This placement follows the intuition that early layers handle pattern recognition while late layers handle reasoning.

\subsection{PEER Module}

Each PEER layer contains:

\begin{itemize}
    \item \textbf{Product keys}: Two sets of $\sqrt{n}$ keys, where $n$ is the number of experts
    \item \textbf{Expert embeddings}: $n$ pairs of down-projection and up-projection vectors
    \item \textbf{Query projection}: Maps hidden states to query vectors for each key set
\end{itemize}

For input hidden state $\mathbf{h} \in \mathbb{R}^d$:

\begin{enumerate}
    \item Project to queries: $\mathbf{q}_1, \mathbf{q}_2 = \text{split}(W_q \mathbf{h})$
    \item Compute similarities: $s_1 = \mathbf{q}_1 K_1^T$, $s_2 = \mathbf{q}_2 K_2^T$
    \item Select top-$k$ from each: $I_1 = \text{topk}(s_1)$, $I_2 = \text{topk}(s_2)$
    \item Form Cartesian product and select final top-$k$ experts
    \item Retrieve expert weights and compute: $\mathbf{o} = \sum_i \alpha_i \cdot \sigma(W_{\text{down}}^i \mathbf{h}) \cdot W_{\text{up}}^i$
\end{enumerate}

With $k=8$ experts per head and 4 heads, we retrieve 32 experts per token from pools of up to 803K experts.

\subsection{Engram Module}

Each Engram layer computes:

\begin{enumerate}
    \item Hash n-grams (2-gram, 3-gram) to indices: $i = \text{hash}(t_{-n+1:0}) \mod V$
    \item Lookup embeddings: $\mathbf{e} = E[i]$
    \item Compute gated output: $\mathbf{o} = \sigma(g(\mathbf{h}, \mathbf{e})) \cdot W_v \mathbf{e}$
\end{enumerate}

The gate $g$ uses the product of normalized keys and queries with sqrt-sign activation for stability.

\subsection{Initialization}

A critical finding: \textbf{new modules must be initialized near-zero} to avoid corrupting pretrained hidden states. We use:

\begin{verbatim}
nn.init.normal_(weight, std=0.01)  # projections
nn.init.normal_(embedding, std=0.02)  # embeddings
\end{verbatim}

Without this, Engram produced 450 perplexity (+2626\%) instead of 11.30 (-31.7\%).

\section{Experiments}

\subsection{Experimental Setup}

\textbf{Base model}: TinyLlama-1.1B-Chat-v1.0 \\
\textbf{Hardware}: NVIDIA RTX 5070 Ti (15.5 GB VRAM) \\
\textbf{Dataset}: WikiText-2 (train and test splits) \\
\textbf{Metrics}: Perplexity, peak VRAM, throughput (tokens/second)

\subsection{Efficiency Benchmark}

We measure inference efficiency across PEER configurations without training (random initialization). This isolates the computational overhead of PEER from any quality effects.

\begin{table}[h]
\centering
\caption{PEER Efficiency Scaling (Untrained)}
\begin{tabular}{lrrrrr}
\toprule
Config & Experts & Parameters & VRAM & Throughput & PPL \\
\midrule
Baseline & — & 1.10B & 2.07 GB & 26,991 tok/s & 17.12 \\
PEER-16K & 16,384 & 1.24B & 2.35 GB & 26,521 tok/s & 17.12 \\
PEER-65K & 65,536 & 1.64B & 3.10 GB & 26,405 tok/s & 17.12 \\
PEER-262K & 262,144 & 3.25B & 6.10 GB & 26,446 tok/s & 17.12 \\
PEER-590K & 589,824 & 5.93B & 11.10 GB & 26,401 tok/s & 17.12 \\
\textbf{PEER-803K} & \textbf{802,816} & \textbf{7.68B} & \textbf{14.35 GB} & \textbf{26,262 tok/s} & \textbf{17.12} \\
\bottomrule
\end{tabular}
\label{tab:efficiency}
\end{table}

\textbf{Key observation}: Throughput remains nearly constant (26,262-26,991 tok/s, $<$3\% variance) despite 7$\times$ parameter increase. Memory scales linearly as expected (expert embeddings are stored), but compute does not—validating PEER's core efficiency claim.

\subsection{Training Results}

We train PEER-262K for 3000 batches with SGD (momentum=0.9, lr=0.01) on WikiText-2 train split.

\begin{table}[h]
\centering
\caption{Quality Results (Trained Models)}
\begin{tabular}{lrrr}
\toprule
Model & Perplexity & Change & Parameters \\
\midrule
Baseline TinyLlama & 17.12 & — & 1.10B \\
+ Engram (trained) & 11.30 & -34.0\% & 1.15B \\
+ PEER-262K (trained) & 14.81 & -13.5\% & 3.25B \\
+ Both (Hybrid) & 11.06 & -35.4\% & 3.30B \\
\bottomrule
\end{tabular}
\label{tab:quality}
\end{table}

\textbf{Key observation}: Trained PEER-262K achieves 14.81 perplexity, a 13.5\% improvement over baseline. This demonstrates that PEER's additional capacity is not just stored—it is \textit{utilized} to improve model quality.

\subsection{Ablation: PEER Layer Placement}

We place PEER in late layers (19, 21) based on the hypothesis that early layers handle pattern matching while late layers handle reasoning. Engram is placed in early layers (1, 5) for complementary coverage.

\subsection{Triple-Sparse Training: The Redundancy Bottleneck}

When training PEER + Engram + mHC jointly, we observe a surprising result: the combined model (-16.6\%) underperforms individual components like Engram (-31.7\%) or mHC (-25.7\%). We term this the \textbf{Redundancy Bottleneck}.

\begin{table}[h]
\centering
\caption{Joint vs Individual Training}
\begin{tabular}{lrr}
\toprule
Configuration & Perplexity & Change \\
\midrule
Baseline & 16.54 & --- \\
Engram alone & 11.30 & -31.7\% \\
mHC alone & 12.29 & -25.7\% \\
PEER alone & 14.81 & -10.5\% \\
\textbf{Triple-Sparse (joint)} & \textbf{13.79} & \textbf{-16.6\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Root cause: Sparsity Overlap.} Analysis of output magnitudes reveals that Engram dominates (2.62:1 ratio vs PEER), effectively ``crowding out'' PEER's contribution. Both modules compete for the same information signal---literal pattern matching.

\subsection{Solution: Differential Learning Rate Training}

We propose \textbf{differential learning rate training} to break the redundancy bottleneck. Instead of freezing modules (which causes distribution shift), we modulate learning rates across phases:

\begin{enumerate}
    \item \textbf{Phase 1 (Engram-Lead)}: Engram LR = $10^{-3}$, PEER LR = $10^{-5}$, mHC LR = $10^{-3}$
    \item \textbf{Phase 2 (PEER-Catchup)}: PEER LR = $10^{-3}$, Engram LR = $10^{-5}$, mHC LR = $10^{-4}$
    \item \textbf{Phase 3 (Coordinate)}: All LR = $5 \times 10^{-4}$
\end{enumerate}

\begin{table}[h]
\centering
\caption{Training Strategy Comparison}
\begin{tabular}{lrrl}
\toprule
Strategy & Perplexity & Change & Notes \\
\midrule
Baseline & 16.54 & --- & --- \\
Joint training & 13.79 & -16.6\% & Redundancy bottleneck \\
Staggered (freeze) & 99.77 & +503\% & Distribution shift \\
\textbf{Differential LR} & \textbf{11.05} & \textbf{-33.2\%} & \textbf{Best result} \\
\bottomrule
\end{tabular}
\label{tab:training_strategies}
\end{table}

\textbf{Key insight}: Never fully freeze modules. Maintaining a minimal learning rate ($10^{-5}$) allows modules to adapt to each other while one ``leads'' and the other ``follows.''

\subsection{The Redundancy Bottleneck in Abstract Terms}

The Redundancy Bottleneck is a special case of a more general phenomenon: \textbf{when multiple modules compete to explain the same signal under a shared loss, the fastest learner wins}. The slower module's gradients diminish as the loss decreases, starving it of learning signal before it can specialize.

This connects to several known failure modes in deep learning:
\begin{itemize}
    \item \textbf{Shortcut learning}: Simple features dominate because they reduce loss faster
    \item \textbf{Representation collapse}: Multiple heads/experts converge to identical functions
    \item \textbf{Early layer dominance}: In deep supervision, early classifiers can starve later ones
\end{itemize}

The mental model: \textit{without coordination, multiple sparse modules act like parallel classifiers racing to minimize loss. The fastest one collapses the error signal, starving the others of gradient.} Differential LR training breaks this race by giving each module a protected learning window.

\section{Analysis}

\subsection{Why Throughput Stays Constant}

PEER's constant throughput despite 7$\times$ parameter increase stems from:

\begin{enumerate}
    \item \textbf{Sparse retrieval}: Only $k=8$ experts activated per head regardless of pool size
    \item \textbf{Product-key efficiency}: $O(\sqrt{n})$ lookup instead of $O(n)$
    \item \textbf{Embedding-based experts}: Simple lookup + matmul, no routing networks
\end{enumerate}

The dominant cost is the base model's attention and MLP computation, which PEER augments rather than replaces.

\subsection{Why Training Improves Quality}

Untrained PEER maintains baseline perplexity because:
\begin{itemize}
    \item Small initialization ($\sigma=0.01$) means PEER output $\approx 0$ initially
    \item The model effectively ignores untrained PEER contributions
\end{itemize}

After training:
\begin{itemize}
    \item Experts specialize to different input patterns
    \item Product-key retrieval routes tokens to relevant experts
    \item The 262K experts provide fine-grained specialization impossible with standard MoE
\end{itemize}

\subsection{Complementary Architectures and Functional Overlap}

Engram and PEER were designed for different purposes:

\begin{itemize}
    \item \textbf{Engram}: Deterministic n-gram lookup, improves pattern recognition
    \item \textbf{PEER}: Learned expert routing, improves contextual reasoning
    \item \textbf{mHC}: Multi-stream residuals with manifold constraints, stabilizes training
\end{itemize}

\textbf{Why mHC matters}: While mHC's -25.7\% improvement is substantial, its mechanism is less obvious than Engram or PEER. We hypothesize that mHC serves as a \textit{representational bandwidth multiplier}: by expanding hidden states into multiple streams with doubly-stochastic mixing, mHC increases the effective dimensionality available for sparse modules to write into. This may prevent early dominance from becoming irreversible---if Engram saturates one stream, PEER can still specialize in another. Gradient analysis shows mHC layers maintain significant gradient flow (0.73 mean magnitude), suggesting they actively participate in optimization rather than merely stabilizing it. A full mechanistic analysis is left for future work, but mHC's consistent contribution across all configurations suggests it plays a coordination role beyond simple residual stabilization.

However, our experiments reveal \textbf{functional overlap} between PEER and Engram. Both modules attempt to ``retrieve'' information based on the input---Engram via literal n-gram hashing, PEER via learned query-key matching. When trained jointly, Engram learns faster (simpler mechanism) and captures patterns that PEER would otherwise specialize in.

\textbf{Evidence of overlap}: Output magnitude analysis shows Engram contributing 2.62$\times$ more than PEER in joint training. Differential LR training forces PEER to specialize in patterns Engram \textit{cannot} capture (context-dependent, non-literal patterns), achieving 11.05 PPL---better than either alone.

\subsection{PEER as Capacity Insurance}

A skeptical reader might ask: ``Why not just use Engram alone?'' The answer reveals PEER's true value proposition.

PEER's immediate quality gain (-10.5\%) is modest relative to its parameter cost (+2.15B). However, PEER provides something Engram cannot: \textbf{compute-decoupled scaling}. Adding more Engram vocabulary increases both memory \textit{and} hash computation. Adding more PEER experts increases only memory---compute stays constant due to product-key retrieval.

This makes PEER \textit{capacity insurance}: a mechanism for future specialization that doesn't bottleneck current inference. In production, this matters enormously. A model can be deployed with 803K experts (7.68B parameters) at the same throughput as a 1.1B model, then continuously trained to utilize that capacity over time.

The Redundancy Bottleneck finding doesn't diminish this---it clarifies \textit{when} PEER's capacity becomes useful. PEER excels at patterns Engram cannot capture: context-dependent combinations, rare sequences, and non-literal associations. Differential LR training simply ensures PEER learns these instead of redundantly duplicating Engram's work.

\section{Discussion}

\subsection{The Broader Principle: Training-Aware Architecture Design}

Our results suggest that architecture and training dynamics are inseparable for multi-component systems. The Redundancy Bottleneck is not a flaw in PEER, Engram, or mHC individually---it emerges from their \textit{interaction during training}.

This has implications beyond our specific architecture:

\begin{enumerate}
    \item Train a moderate base model (1-7B parameters)
    \item Add PEER layers with millions of experts
    \item Train only PEER parameters (faster, less memory)
    \item Result: Quality of larger model, inference cost of smaller model
\end{enumerate}

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Dataset bias}: WikiText-2's repetitive n-gram structure may inflate Engram's contribution. Cross-dataset evaluation on C4 shows Engram's gains diminish on diverse web content (+0.6\% vs -31.7\% on WikiText-2), suggesting the -31.7\% improvement is partially dataset-specific. The Redundancy Bottleneck phenomenon itself generalizes, but optimal differential LR schedules may be dataset-dependent.
    \item \textbf{Memory still scales}: While compute is constant, VRAM scales linearly with experts
    \item \textbf{Small base model}: TinyLlama (1.1B) may not represent behavior at larger scales
    \item \textbf{PEER as capacity insurance}: PEER's immediate quality gain (-10.5\%) is modest relative to its parameter cost (+2.15B). Its value is \textit{scaling headroom}---the ability to add capacity without proportional compute increase---rather than immediate perplexity improvement. This is a feature, not a bug, but users expecting immediate gains may be disappointed.
    \item \textbf{Differential LR sensitivity}: The phase durations (1000 batches each) and LR ratios (1e-3 vs 1e-5) are hand-tuned for this architecture. These hyperparameters likely require re-tuning for different model sizes, datasets, and module combinations.
\end{itemize}

\subsection{Future Work}

\begin{itemize}
    \item Scale to 1M+ experts with CPU offloading or quantization
    \item Evaluate on diverse benchmarks (MMLU, HellaSwag, etc.)
    \item Compare PEER training efficiency vs full model training
    \item Investigate optimal layer placement and expert count
\end{itemize}

\section{Conclusion}

This paper's core finding is simple but consequential: \textbf{sparse retrieval modules compete for representational real estate}. When multiple sparse mechanisms are trained jointly, the faster-learning module crowds out the slower one, leading to a Redundancy Bottleneck where the combined system underperforms its components.

The solution is equally simple: \textbf{coordinate training dynamics}. Differential learning rate training---where modules take turns leading optimization---achieves -33.2\% perplexity, better than any individual component or naive joint training. The same architecture that produces 99.77 PPL with frozen-phase training achieves 11.05 PPL with differential LR. Architecture is not destiny; training dynamics determine outcomes.

Our findings generalize beyond the specific PEER/Engram/mHC combination:
\begin{enumerate}
    \item \textbf{Any sparse retrieval system} combining multiple mechanisms (MoE routing, hash-based memory, learned retrieval) should expect functional overlap
    \item \textbf{Output magnitude analysis} can diagnose which module dominates and by how much
    \item \textbf{Differential LR training} provides a general coordination principle: let modules specialize sequentially rather than competing simultaneously
\end{enumerate}

We also validate PEER's efficiency claims: 7$\times$ parameter scaling with $<$3\% throughput reduction. PEER's value is capacity insurance---scaling headroom that doesn't bottleneck inference.

The broader implication is that \textit{training-aware architecture design} is essential. Evaluating architectures without considering training dynamics misses half the story. The Redundancy Bottleneck is invisible if you only test individual components; it only emerges when components interact during joint optimization.

\section*{Code Availability}

Code and trained weights are available at: \url{https://github.com/MikeyBeez/PEER}

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{peer2024}
Anonymous Authors.
\textit{Mixture of A Million Experts}.
arXiv:2407.04153, 2024.

\bibitem{engram2024}
DeepSeek AI.
\textit{Engram: Byte-Level Memory-Augmented Language Modeling}.
GitHub, 2024.

\bibitem{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.
\textit{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}.
ICLR, 2017.

\bibitem{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\textit{GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding}.
ICLR, 2021.

\bibitem{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\textit{Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}.
JMLR, 2022.

\bibitem{lample2019large}
Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou.
\textit{Large Memory Layers with Product Keys}.
NeurIPS, 2019.

\bibitem{mhc2024}
DeepSeek AI.
\textit{Manifold-Constrained Hyper-Connections for Stable Deep Learning}.
arXiv:2512.24880, 2024.

\end{thebibliography}

\appendix

\section{mHC: Manifold-Constrained Hyper-Connections}
\label{app:mhc}

In addition to PEER and Engram, we implement mHC (Manifold-constrained Hyper-Connections) \cite{mhc2024}, which replaces standard residual connections with multi-stream hyper-connections. This creates a \textit{triple-sparse architecture} where each component serves a distinct purpose.

\subsection{mHC Architecture}

mHC expands hidden states into $N$ parallel streams and uses learned mixing matrices projected to doubly-stochastic form via Sinkhorn-Knopp iteration.

For input $\mathbf{x} \in \mathbb{R}^{B \times T \times D}$:

\begin{enumerate}
    \item \textbf{Stream Expansion}: $\mathbf{x}_{\text{exp}} = W_{\text{expand}} \mathbf{x} \in \mathbb{R}^{B \times T \times N \times D}$
    \item \textbf{Compute mixing weights} from flattened input:
    \begin{itemize}
        \item Pre-mixing: $W_{\text{pre}} = \sigma(\alpha_{\text{pre}} + \tanh(P_{\text{pre}} \mathbf{x}_{\text{flat}}))$
        \item Post-mixing: $W_{\text{post}} = 2\sigma(\alpha_{\text{post}} + \tanh(P_{\text{post}} \mathbf{x}_{\text{flat}}))$
        \item Residual mixing: $W_{\text{res}} = \text{Sinkhorn}(\exp(P_{\text{res}} \mathbf{x}_{\text{flat}}))$
    \end{itemize}
    \item \textbf{Apply inner function} (attention or MLP): $\mathbf{y} = f(W_{\text{pre}} \mathbf{x}_{\text{exp}})$
    \item \textbf{Combine}: $\mathbf{o} = W_{\text{res}} \mathbf{x}_{\text{exp}} + W_{\text{post}} \mathbf{y}$
    \item \textbf{Stream Reduction}: $\mathbf{x}_{\text{out}} = W_{\text{reduce}} \mathbf{o} \in \mathbb{R}^{B \times T \times D}$
\end{enumerate}

\subsection{Sinkhorn-Knopp Projection}

The residual mixing matrix $W_{\text{res}}$ is projected to doubly-stochastic form (rows and columns sum to 1) via iterative normalization:

\begin{algorithm}
\caption{Sinkhorn-Knopp Projection}
\begin{algorithmic}[1]
\REQUIRE Matrix $A \in \mathbb{R}^{N \times N}$, iterations $K$
\STATE $A \leftarrow \exp(A)$ \COMMENT{Ensure non-negative}
\FOR{$k = 1$ to $K$}
    \STATE $A \leftarrow A / \sum_j A_{ij}$ \COMMENT{Row normalization}
    \STATE $A \leftarrow A / \sum_i A_{ij}$ \COMMENT{Column normalization}
\ENDFOR
\RETURN $A$
\end{algorithmic}
\end{algorithm}

Doubly-stochastic matrices preserve signal magnitude, preventing gradient explosion/vanishing in deep networks.

\subsection{mHC Integration Results}

We apply mHC to layers 5, 11, and 17 (spread across the 22-layer model) with $N=4$ streams.

\begin{table}[h]
\centering
\caption{mHC Training Results}
\begin{tabular}{lrrr}
\toprule
Model & Perplexity & Change & Parameters \\
\midrule
Baseline TinyLlama & 16.54 & — & 1.10B \\
+ mHC (trained) & 12.29 & \textbf{-25.7\%} & 1.21B \\
\bottomrule
\end{tabular}
\label{tab:mhc}
\end{table}

\textbf{Key finding}: mHC achieves \textbf{25.7\% perplexity improvement}, demonstrating that multi-stream residual connections with manifold constraints significantly enhance model quality.

\subsection{mHC Implementation Details}

Our mHC wrapper applies processing \textit{after} the decoder layer rather than replacing it:

\begin{verbatim}
class mHCLayer(nn.Module):
    def forward(self, hidden_states):
        x_exp = self.expander(hidden_states)
        x_mhc = self.mhc(x_exp)
        x_red = self.reducer(x_mhc)
        mhc_out = self.out_proj(x_red)
        gate = sigmoid(self.gate_scale)  # ~0.018
        return hidden_states + gate * mhc_out
\end{verbatim}

Critical design choices:
\begin{itemize}
    \item \textbf{Gated output}: Gate initialized to $\sim$0.018 via \texttt{gate\_scale = -4.0}
    \item \textbf{Small projection init}: \texttt{std=0.001} for output projection
    \item \textbf{Post-layer application}: mHC processes layer output, not input
\end{itemize}

\section{Complete Results Summary}
\label{app:results}

\begin{table}[h]
\centering
\caption{All Training Results on WikiText-2}
\begin{tabular}{lrrrr}
\toprule
Model & Perplexity & Change & Params & Throughput \\
\midrule
Baseline TinyLlama & 16.54 & — & 1.10B & 26,991 tok/s \\
\midrule
\multicolumn{5}{l}{\textit{Single Components}} \\
+ Engram & 11.30 & -31.7\% & 1.15B & $\sim$26K tok/s \\
+ PEER-262K & 14.81 & -10.5\% & 3.25B & 26,446 tok/s \\
+ mHC & 12.29 & -25.7\% & 1.21B & $\sim$25K tok/s \\
\midrule
\multicolumn{5}{l}{\textit{Dual Combinations}} \\
+ Engram + PEER & 11.06 & -33.1\% & 3.30B & $\sim$26K tok/s \\
\midrule
\multicolumn{5}{l}{\textit{Triple-Sparse (PEER + Engram + mHC)}} \\
Joint training & 13.79 & -16.6\% & 3.41B & $\sim$25K tok/s \\
Staggered (freeze) & 99.77 & +503\% & 3.41B & --- \\
\textbf{Differential LR} & \textbf{11.05} & \textbf{-33.2\%} & \textbf{3.41B} & $\sim$25K tok/s \\
\bottomrule
\end{tabular}
\label{tab:all_results}
\end{table}

\section{Training Hyperparameters}
\label{app:hyperparams}

\begin{table}[h]
\centering
\caption{Training Hyperparameters}
\begin{tabular}{lccc}
\toprule
Parameter & Engram & PEER-262K & mHC \\
\midrule
Optimizer & AdamW & SGD & AdamW \\
Learning rate (new params) & $10^{-3}$ & $10^{-2}$ & $10^{-3}$ \\
Learning rate (backbone) & $10^{-6}$ & $10^{-8}$ & $10^{-6}$ \\
Momentum & 0.9 (Adam $\beta$) & 0.9 & 0.9 (Adam $\beta$) \\
Weight decay & 0.01 & 0 & 0.01 \\
Batch size & 4 & 4 & 4 \\
Sequence length & 256 & 256 & 256 \\
Training batches & 1,500 & 3,000 & 1,500 \\
Gradient clipping & 1.0 & 1.0 & 1.0 \\
Precision & bfloat16 & bfloat16 & bfloat16 \\
\bottomrule
\end{tabular}
\label{tab:hyperparams}
\end{table}

\subsection{Why Different Optimizers?}

PEER-262K uses SGD instead of AdamW because:
\begin{itemize}
    \item AdamW maintains 2$\times$ parameter memory for optimizer states
    \item With 262K experts (2.15B additional parameters), this exhausted 15.5GB VRAM
    \item SGD with momentum requires only 1$\times$ additional memory
\end{itemize}

\subsection{Differential LR Training Schedule}

\begin{table}[h]
\centering
\caption{Differential Learning Rate Training Phases}
\begin{tabular}{lcccc}
\toprule
Phase & Batches & Engram LR & PEER LR & mHC LR \\
\midrule
1. Engram-Lead & 1,000 & $10^{-3}$ & $10^{-5}$ & $10^{-3}$ \\
2. PEER-Catchup & 1,000 & $10^{-5}$ & $10^{-3}$ & $10^{-4}$ \\
3. Coordinate & 1,000 & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ \\
\bottomrule
\end{tabular}
\label{tab:diff_lr}
\end{table}

\textbf{Key insight}: The minimum learning rate of $10^{-5}$ (not zero) is critical. Completely freezing modules causes distribution shift, leading to catastrophic forgetting (99.77 PPL). Maintaining a small learning rate allows modules to adapt to each other's changing outputs while one ``leads'' the optimization.

\section{Initialization Details}
\label{app:init}

Proper initialization is critical when augmenting pretrained models. Default PyTorch initialization produces high-variance outputs that corrupt carefully tuned hidden states.

\begin{table}[h]
\centering
\caption{Initialization Standard Deviations}
\begin{tabular}{lcc}
\toprule
Component & Weight Type & Std Dev \\
\midrule
\multicolumn{3}{l}{\textit{PEER}} \\
Query projection & Linear & 0.02 \\
Product keys & Parameter & 0.02 \\
Expert embeddings & Embedding & 0.01 \\
\midrule
\multicolumn{3}{l}{\textit{Engram}} \\
Value projection & Linear & 0.01 \\
Key projection & Linear & 0.01 \\
N-gram embeddings & Embedding & 0.02 \\
Convolution & Conv1d & 0 (zero init) \\
\midrule
\multicolumn{3}{l}{\textit{mHC}} \\
Stream expander & Linear & 0.02 \\
Stream reducer & Linear & 0.02 \\
$\alpha$ parameters & Scalar & 0.01 \\
Mixing projections & Linear & 0.01 \\
Output projection & Linear & 0.001 \\
Gate scale & Scalar & -4.0 (logit) \\
\bottomrule
\end{tabular}
\label{tab:init}
\end{table}

\subsection{Initialization Ablation}

\begin{table}[h]
\centering
\caption{Effect of Initialization on Engram}
\begin{tabular}{lrr}
\toprule
Initialization & Perplexity & Change \\
\midrule
Default PyTorch & 450.0 & +2,621\% \\
Small init (std=0.01-0.02) & 11.30 & -31.7\% \\
\bottomrule
\end{tabular}
\label{tab:init_ablation}
\end{table}

\section{PEER Module Algorithm}
\label{app:peer_algorithm}

\begin{algorithm}
\caption{PEER Forward Pass}
\begin{algorithmic}[1]
\REQUIRE Hidden state $\mathbf{h} \in \mathbb{R}^{B \times T \times D}$
\REQUIRE Keys $K_1, K_2 \in \mathbb{R}^{H \times \sqrt{n} \times d_k}$
\REQUIRE Expert embeddings $E_{\text{down}}, E_{\text{up}} \in \mathbb{R}^{n \times D}$
\STATE $\mathbf{q} \leftarrow W_q \mathbf{h}$ \COMMENT{Project to queries}
\STATE $\mathbf{q}_1, \mathbf{q}_2 \leftarrow \text{split}(\mathbf{q})$ \COMMENT{Split for product keys}
\FOR{each head $h$}
    \STATE $s_1 \leftarrow \mathbf{q}_1^h (K_1^h)^T$ \COMMENT{Similarities to key set 1}
    \STATE $s_2 \leftarrow \mathbf{q}_2^h (K_2^h)^T$ \COMMENT{Similarities to key set 2}
    \STATE $I_1, v_1 \leftarrow \text{topk}(s_1, k)$ \COMMENT{Top-k indices and scores}
    \STATE $I_2, v_2 \leftarrow \text{topk}(s_2, k)$
    \STATE $I_{\text{prod}} \leftarrow I_1 \times \sqrt{n} + I_2$ \COMMENT{Cartesian product}
    \STATE $v_{\text{prod}} \leftarrow v_1 + v_2$ \COMMENT{Combined scores}
    \STATE $I_{\text{final}}, v_{\text{final}} \leftarrow \text{topk}(I_{\text{prod}}, v_{\text{prod}}, k)$
    \STATE $W_{\text{down}} \leftarrow E_{\text{down}}[I_{\text{final}}]$
    \STATE $W_{\text{up}} \leftarrow E_{\text{up}}[I_{\text{final}}]$
    \STATE $\mathbf{a} \leftarrow \text{ReLU}(v_{\text{final}})$ \COMMENT{Non-competing scores}
    \STATE $\mathbf{o}^h \leftarrow \sum_i \mathbf{a}_i \cdot \sigma(W_{\text{down}}^i \mathbf{h}) \cdot W_{\text{up}}^i$
\ENDFOR
\RETURN $\sum_h \mathbf{o}^h$
\end{algorithmic}
\end{algorithm}

\section{Engram Module Algorithm}
\label{app:engram_algorithm}

\begin{algorithm}
\caption{Engram Forward Pass}
\begin{algorithmic}[1]
\REQUIRE Hidden state $\mathbf{h} \in \mathbb{R}^{B \times T \times D}$
\REQUIRE Input token IDs $\mathbf{t} \in \mathbb{Z}^{B \times T}$
\REQUIRE N-gram embeddings $E_2, E_3 \in \mathbb{R}^{V \times d_e}$
\STATE \COMMENT{Hash 2-grams}
\STATE $i_2 \leftarrow (\mathbf{t} \oplus (\mathbf{t}_{\text{shift}} \times m_1)) \mod V$
\STATE $\mathbf{e}_2 \leftarrow E_2[i_2]$
\STATE \COMMENT{Hash 3-grams}
\STATE $i_3 \leftarrow (i_2 \oplus (\mathbf{t}_{\text{shift2}} \times m_2)) \mod V$
\STATE $\mathbf{e}_3 \leftarrow E_3[i_3]$
\STATE $\mathbf{e} \leftarrow \text{concat}(\mathbf{e}_2, \mathbf{e}_3)$
\STATE \COMMENT{Compute gate}
\STATE $\mathbf{k} \leftarrow W_k \mathbf{e}$
\STATE $g \leftarrow (\text{RMSNorm}(\mathbf{k}) \cdot \text{RMSNorm}(\mathbf{h})) / \sqrt{D}$
\STATE $g \leftarrow \sigma(\sqrt{|g|} \cdot \text{sign}(g))$ \COMMENT{Sqrt-sign activation}
\STATE \COMMENT{Gated output}
\STATE $\mathbf{v} \leftarrow g \cdot W_v \mathbf{e}$
\STATE $\mathbf{v} \leftarrow \mathbf{v} + \text{SiLU}(\text{Conv1d}(\mathbf{v}))$
\RETURN $\mathbf{h} + \mathbf{v}$
\end{algorithmic}
\end{algorithm}

\section{Hardware and Reproducibility}
\label{app:hardware}

\begin{table}[h]
\centering
\caption{Hardware Configuration}
\begin{tabular}{ll}
\toprule
Component & Specification \\
\midrule
GPU & NVIDIA RTX 5070 Ti \\
VRAM & 15.5 GB GDDR7 \\
CUDA Version & 12.x \\
PyTorch Version & 2.x \\
Transformers & 4.x \\
\bottomrule
\end{tabular}
\label{tab:hardware}
\end{table}

\subsection{Reproducibility Checklist}

\begin{itemize}
    \item[$\checkmark$] Random seeds set for reproducibility
    \item[$\checkmark$] All hyperparameters documented
    \item[$\checkmark$] Training/evaluation code publicly available
    \item[$\checkmark$] Pretrained weights available for download
    \item[$\checkmark$] Dataset (WikiText-2) publicly available
\end{itemize}

\section{Limitations and Future Work}
\label{app:limitations}

\subsection{Current Limitations}

\begin{enumerate}
    \item \textbf{Single evaluation dataset}: WikiText-2 is n-gram heavy, which may favor Engram
    \item \textbf{Small base model}: TinyLlama (1.1B) may not represent larger model behavior
    \item \textbf{No downstream task evaluation}: Only perplexity measured, not task performance
    \item \textbf{Differential LR sensitivity}: Optimal phase durations and LR ratios may vary across architectures
    \item \textbf{Limited training budget}: Longer training may yield further improvements
\end{enumerate}

\subsection{Future Directions}

\begin{enumerate}
    \item \textbf{Downstream tasks}: Evaluate on MMLU, HellaSwag, ARC, etc.
    \item \textbf{Larger base models}: Test on 7B+ parameter models
    \item \textbf{Expert scaling}: Push to 1M+ experts with memory optimization
    \item \textbf{Continuous pretraining}: Train on larger corpora (RedPajama, etc.)
    \item \textbf{Differential LR optimization}: Tune phase durations and LR schedules for different architectures
    \item \textbf{Redundancy analysis}: Develop automated methods to detect and prevent module overlap
\end{enumerate}

\end{document}
